import json
import os
import re
import time
import random
from datetime import datetime
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from pymongo import MongoClient

# === Chrome + TOR ì„¤ì • ===
chromedriver_path = "C:\\DEEP_DIVE\\TorCrawling\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe"

proxy_address = "127.0.0.1:9150"

chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument(f"--proxy-server=socks5://{proxy_address}")

service = Service(chromedriver_path)
driver = webdriver.Chrome(service=service, options=chrome_options)

# === MongoDB ì—°ê²° ===
mongo_uri = "mongodb+srv://lch5159:dl2ckd3gus3@ransomcrawl.mmnwun3.mongodb.net/?retryWrites=true&w=majority&appName=RansomCrawl"
mongo_client = MongoClient(mongo_uri)
mongo_db = mongo_client['ransomware_db']
mongo_collection = mongo_db['detect']

# === JSON ì €ì¥ ê²½ë¡œ ì„¤ì • ===
base_dir = os.path.dirname(os.path.abspath(__file__))
detect_dir = os.path.join(base_dir, 'detect')
if not os.path.exists(detect_dir):
    os.makedirs(detect_dir)

# ì¤‘ë³µ ë°©ì§€ìš© title ìºì‹œ
last_processed_titles = set()

# JSON ë²ˆí˜¸ ì¦ê°€
def get_next_detect_dir_number():
    files = [f for f in os.listdir(detect_dir) if re.match(r"detect_(\d+)\.json", f)]
    max_num = 0
    for f in files:
        m = re.match(r"detect_(\d+)\.json", f)
        if m:
            num = int(m.group(1))
            if num > max_num:
                max_num = num
    return max_num + 1

# MongoDB ì—…ë¡œë“œ í•¨ìˆ˜
def upload_to_mongodb(data):
    try:
        # ê¸°ì¡´ ë°ì´í„° ì¤‘ë³µ ì²´í¬
        existing = mongo_collection.find_one({
            'title': data['title'],
            'group': data['group'],
            'date': data['date']
        })
        
        if not existing:
            data['processed'] = False  # ì•Œë¦¼ ì²˜ë¦¬ ì—¬ë¶€ í”Œë˜ê·¸
            result = mongo_collection.insert_one(data)
            print(f"âœ… MongoDB ì—…ë¡œë“œ ì™„ë£Œ (ID: {result.inserted_id})")
            return result.inserted_id
        else:
            print("â© ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë°ì´í„° (ì¤‘ë³µ ê±´ë„ˆëœ€)")
            return existing['_id']
    except Exception as e:
        print(f"âŒ MongoDB ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# ê·¸ë£¹ ì •ë³´ í¬ë¡¤ë§
def crawl_detect_dir(group):
    if group == "j group":
        group_name = "J"
    else:
        group_name = group.replace(" ", "").lower()

    url = f"https://www.ransomware.live/group/{group_name}"
    driver.get(url)
    time.sleep(5)
    soup = BeautifulSoup(driver.page_source, "html.parser")

    summary = soup.find("div", class_="d-flex justify-content-around my-4 flex-wrap gap-3")
    victims_count = last_victim = first_victim = avg_delay = infostealer = ""

    if summary:
        cards = summary.find_all("div", class_="bg-light")
        for card in cards:
            h6 = card.find("h6")
            value = (card.find("h3") or card.find("h4")).get_text(strip=True) if card.find("h3") or card.find("h4") else ""
            if h6:
                title = h6.get_text(strip=True)
                if "Victims" in title:
                    try: victims_count = int(value)
                    except: victims_count = value
                elif "First Discovered" in title: first_victim = value
                elif "Last Discovered" in title: last_victim = value
                elif "Avg Delay" in title: avg_delay = value
                elif "Infostealer" in title: infostealer = value

    target_data = []
    target_section = soup.find(id="target-section")
    if target_section:
        for card in target_section.find_all("div", class_="card"):
            header = card.find("div", class_="card-header")
            if not header: continue
            title = header.get_text(strip=True)
            items = card.find_all("li", class_="list-group-item")
            data_dict = {}
            for item in items:
                spans = item.find_all("span")
                if len(spans) >= 2:
                    key = spans[0].get_text(strip=True)
                    try: value = int(spans[1].get_text(strip=True))
                    except: value = ""
                    data_dict[key] = value
            target_data.append({title: [data_dict]})

    victims = []
    victim_list = soup.find(id="victim-list")
    if victim_list:
        for v in victim_list.find_all("div", class_="d-flex flex-column text-start flex-grow-1"):
            v_name = v.find("a", class_="text-body-emphasis text-decoration-none")
            v_name = v_name.get_text(strip=True) if v_name else ""
            discovery_date = estimated_attack_date = ""
            date_div = v.find("div", class_="text-body-secondary mt-2")
            if date_div:
                for strong in date_div.find_all("strong"):
                    label = strong.get_text(strip=True)
                    sibling = strong.next_sibling
                    if label == "Discovery Date:" and sibling:
                        discovery_date = str(sibling).strip()
                    elif label == "Estimated Attack Date:" and sibling:
                        estimated_attack_date = str(sibling).strip()
            desc_div = v.find("div", class_="p-2 rounded shadow-sm text-body-secondary bg-body-secondary")
            v_description = desc_div.get_text(strip=True) if desc_div else ""
            victims.append({
                "v_name": v_name,
                "Discovery Date": discovery_date,
                "Estimated Attack Date": estimated_attack_date,
                "v_description": v_description
            })

    return {
        "Victims count": victims_count,
        "Last discovered victim": last_victim,
        "First discovered victims": first_victim,
        "Avg Delay": avg_delay,
        "Infostealer": infostealer,
        "Target": target_data,
        "victims": victims
    }

# ìµœì‹  title ì¶”ì¶œ
def get_latest_titles(n=100):
    driver.get("http://ransomlookumjrc6erzqn467lkcu2t5h4enjzfigvsxrrktxicysi2yd.onion/recent")
    time.sleep(5)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    rows = soup.find_all("tr")
    titles = []
    for row in rows[1:n+1]:
        tds = row.find_all("td")
        if len(tds) > 1:
            titles.append(tds[1].get_text(strip=True))
    return titles

# ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_files():
    global last_processed_titles
    driver.get("http://ransomlookumjrc6erzqn467lkcu2t5h4enjzfigvsxrrktxicysi2yd.onion/recent")
    time.sleep(5)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    rows = soup.find_all("tr")

    if len(rows) <= 1:
        print("âŒ í…Œì´ë¸”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    post_datas = []
    for row in rows[1:101]:
        tds = row.find_all("td")
        if len(tds) > 1:
            date_str = tds[0].get_text(strip=True)
            title = tds[1].get_text(strip=True)
            group = tds[2].find("a").get_text(strip=True) if len(tds) > 2 and tds[2].find("a") else ""
            if title in last_processed_titles:
                continue

            detect_dir_data = crawl_detect_dir(group)
            post_data = {
                "date": date_str,
                "title": title,
                "group": group,
                **detect_dir_data,
                "crawled_at": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            post_datas.append(post_data)

    if post_datas:
        for post in post_datas:
            num = get_next_detect_dir_number()
            filename = os.path.join(detect_dir, f"detect_{num}.json")
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(post, f, ensure_ascii=False, indent=4)
            print(f"ğŸ“ ì €ì¥ ì™„ë£Œ: {filename}")
            upload_to_mongodb(post)
    else:
        print("ìƒˆë¡œìš´ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")

    last_processed_titles.update(get_latest_titles(100))

# === ë©”ì¸ ë£¨í”„ ===
if __name__ == "__main__":
    try:
        last_processed_titles = set(get_latest_titles(100))
        print("ğŸ”„ ì‹œì‘í•©ë‹ˆë‹¤.")
        while True:
            crawl_files()
            wait = random.randint(30, 90)
            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            print(f"\nâ³ {current_time} ê¸°ì¤€, {wait}ì´ˆ í›„ ë‹¤ìŒ í¬ë¡¤ë§...")

            time.sleep(wait)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ì‚¬ìš©ì ì¢…ë£Œ")
    finally:
        driver.quit()
        mongo_client.close()
